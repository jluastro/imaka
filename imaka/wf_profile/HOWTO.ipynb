{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Run\n",
    "## A guide to getting started with WFP\n",
    "This Jupyter notebook should give a comprehensive overview for figuring out how to run the imaka wavefront profiler. \n",
    "\n",
    "**Contents:**\n",
    "- Required Python Libraries\n",
    "- Configuration Files\n",
    "- Run Files\n",
    "- Target Files\n",
    "- Running Correlation Pipeline (Basics)\n",
    "- Running Estimation Pipeline (Basics) TBD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Required Python Libraries\n",
    "Below is an import list of all functions used throughout this code. If you're able to run the following block of code, you should be able to run this package without issues.\n",
    "\n",
    "!TODO: check that all libraries are used throughout code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "import os\n",
    "import sys\n",
    "import fnmatch\n",
    "import logging\n",
    "import threading\n",
    "import time\n",
    "import concurrent.futures\n",
    "#Correlator\n",
    "import math\n",
    "import imageio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import readsav\n",
    "import matplotlib\n",
    "from astropy.table import Table\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.io import fits\n",
    "#Plotting\n",
    "from celluloid import Camera\n",
    "from IPython.display import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports are from seperate packages that aren't necessary for core functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datatable: not required for corr code\n",
    "from datetime import datetime, date, time, timezone\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator\n",
    "import hdbscan\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Configuration Files\n",
    "Configuration files consist of the directory locations you wish the correlator function to find and store data. \n",
    "\n",
    "Location:\n",
    "`wf_profile/inputs/conf/`\n",
    "\n",
    "### Defining a variable path: \n",
    "name of variable a space, an equals, a space, value, line break. This is the order that the file reader will parse inputs.\n",
    "\n",
    "> name = value\n",
    "\n",
    "The file reader for this file will not catch anything that starts with \"#\" so feel free to use those as comments.\n",
    "\n",
    "### The major variables to edit: \n",
    "The big variables to edit are the paths specific to your system. They are described below:\n",
    "\n",
    "> `data_path = /home/imaka/data/`\n",
    "\n",
    "The data path shows the system where to look for files that contain dates. for example, this is the directory where we'd expect to find **/home/imaka/data/**20201102\n",
    "\n",
    "> `out_path = /home/emcewen/out/`\n",
    "\n",
    "This is where you'll be storing your data output. The system will create this path if it can, so be sure you're pointing it in the right direction\n",
    "\n",
    "> `target_path = /home/emcewen/data/target_input/`\n",
    "\n",
    "The target path shows the system where to look for target files. In this package, target files are listed under: **wf_profile/inputs/targets**\n",
    "\n",
    "\n",
    "### Varibles that don't need changing:\n",
    "These are ways to customize how the pipeline operates or what it outputs. Don't feel the need to change them for a typical run. \n",
    "\n",
    "> `pipe_fn = all`\n",
    "\n",
    "This determines whether you want just data fits, plots from previous files, or both at once. **all** should work for most users.\n",
    "\n",
    "> `parallel = True`\n",
    "\n",
    "This allows you to do a parallization of the conf run, but is experimental."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Configure file for DATA\n",
      "# => Change data paths here\n",
      "\n",
      "# Commented lines will be ignored\n",
      "# variables must be defined with \" = \" (spaces included)\n",
      "\n",
      "# where to place files\n",
      "data_path = /home/imaka/data/\n",
      "out_path = /home/emcewen/out/\n",
      "target_path = /home/emcewen/data/target_input/\n",
      "\n",
      "# options: all, data, plots\n",
      "pipe_fn = all\n",
      "\n",
      "# Parallel processing\n",
      "parallel = True\n"
     ]
    }
   ],
   "source": [
    "# Example configuration file:\n",
    "file = 'inputs/conf/CONF_data.txt'\n",
    "print(open(file, 'r').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Conf Varibles:\n",
    "You can access these files with a built in function in `pipeline/code/file_reader.py`.\n",
    "\n",
    "This is helpful if you ever need to pull from one of these files in your own code building off of the WFS profiler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/imaka/data/'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in conf variables\n",
    "from pipeline.code.file_reader import *\n",
    "\n",
    "conf_f = 'inputs/conf/CONF_data.txt'\n",
    "conf_d = read_file_dic(conf_f)\n",
    "\n",
    "conf_d[\"data_path\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Run files\n",
    "\n",
    "Run files are inputs to the main pipeline that outlines either the dates or the files for the correlator to run over. \n",
    "\n",
    "Entries can be:\n",
    "- **a date**, will be looked for in **data_path** and all fits files in date/ or date/ao will be added to a list of files to process\n",
    "- **target name**, will look for this file in the **target_path** defined in conf, where the file begins with **target_name** and ends with **_USNOB_GS.txt**. If a new target is inserted, all dates after the change will be given that target until switched again. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beehive-W\n",
      "20190226\n"
     ]
    }
   ],
   "source": [
    "# Example Run File: Data\n",
    "file = 'inputs/runs/RUN10.txt'\n",
    "print(open(file, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simul_4GS_12am_6mag\n",
      "20200904\n"
     ]
    }
   ],
   "source": [
    "# Example Run File: Simulation\n",
    "file = 'inputs/runs/RUN_simul.txt'\n",
    "print(open(file, 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Target files\n",
    "\n",
    "Target files are optional, but will give more information to analysis. They give information on the targets of each individual WFS in configurations.\n",
    "\n",
    "**Information included:**\n",
    "- **file name:** first line (without the target suffix identifier)\n",
    "- **number of wfs:** how many wfs are specifically used.\n",
    "- **WFS entries:** WFS number, RA, DEC, and Mag, tab seperated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beehive-W\n",
      "5                      \n",
      "0\t129.961317\t19.540806\t6.0\n",
      "3\t130.112547\t19.544806\t6.2\n",
      "4\t130.092028\t19.669939\t5.9\n",
      "1\t129.927728\t19.778453\t6.5\n",
      "2\t130.180028\t19.719314\t6.7\n"
     ]
    }
   ],
   "source": [
    "# Example Run File: Beehive-W\n",
    "\n",
    "file = open('inputs/targets/Beehive-W_USNOB_GS.txt', 'r')\n",
    "print (file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simul.4GS.12am.6mag\n",
      "4\n",
      "0\t6\t0\t6\n",
      "1\t0\t6\t6\n",
      "2\t-6\t0\t6\n",
      "3\t0\t-6\t6\n"
     ]
    }
   ],
   "source": [
    "# Example Run File: Simul\n",
    "\n",
    "file = open('inputs/targets/simul_4GS_12am_6mag_USNOB_GS.txt', 'r')\n",
    "print (file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Running Correlation Pipeline (Basics)\n",
    "\n",
    "Wavefront profiler correlation pipeline Location:\n",
    "\n",
    "`wf_profile/pipeline/wfp_pipeline`\n",
    "\n",
    "The correlation pipeline is the main pipeline for this code package. Below we will explain the steps taken with a run. You should have ready the file path for the configuration file you want to use and the run file you'd like to include.\n",
    "\n",
    "**A pipeline call configuration**\n",
    "\n",
    "Structure:\n",
    "\n",
    "> `python3 [complete path to wf_profile] [CONF_file with path] [RUN_file with path]`\n",
    "\n",
    "Exaple call:\n",
    "\n",
    ">` python3 pipeline/wf_pipeline inputs/CONF.txt inputs/RUN.txt`\n",
    "\n",
    "Note that this call is run from the `wf_profile` folder. Be sure to include the paths from your current folder to the conf file and run files that you wish.\n",
    "\n",
    "Run from the terminal. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## => Running Estimation Pipeline (Basics)\n",
    "\n",
    "as of right now, there is no Basic way to use the Estimation pipeline. Please see the corresponding howto document for a walk through example on how to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
